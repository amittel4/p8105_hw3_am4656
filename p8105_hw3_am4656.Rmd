---
title: "Homework 3"
author: "Aaron Mittel"
date: "2022-10-15"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(p8105.datasets)
library(viridis)
library(lubridate)
data("instacart")

instacart =
  instacart %>%
  as_tibble(instacart)

ny_noaa =
  ny_noaa %>% 
  as_tibble(ny_noaa)
```

# Problem 1

* The instacart dataset includes **`r nrow(instacart)` rows** and **`r ncol(instacart)` columns**.
* There are **`r instacart %>% select(user_id) %>% distinct %>% count` distinct users** who have ordered a total of **`r instacart %>% select(order_id) %>% count` orders** among **`r instacart %>% select(product_id) %>% distinct %>% count` distinct products**.
* Key variables include: order_id, product_id, user_id, order_number, order_hour_of_day, days_since_prior_orer, aisle_id, and department_id.
* There are **`r nrow(instacart %>% count(aisle_id))` aisles**.
* In Table 1 below we can see that the most items are ordered from Aisle 83, with a total of 150,609 items ordered.
```{r Table 1, echo = FALSE}
instacart %>% 
  count(aisle_id) %>% 
  arrange(desc(n)) %>%
  filter(n > 50000) %>%
  knitr::kable()
```
**Table 1**

* Plot indicating number of items in each aisle:
```{r, echo = FALSE}
instacart %>% 
  count(aisle) %>%
  filter(n > 10000) %>%
  mutate(aisle = fct_reorder(aisle, n)) %>%
  ggplot(aes(x = aisle, y = n)) +
  geom_col() + 
  labs(title = "Count of Items Ordered in Each Aisle") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


* Table 2, below, shows the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”, including the number of times each item is ordered.
```{r, echo = FALSE}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>%
  knitr::kable()
```
**Table 2**


* Table 3, below, shows the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week, formatted for human readers (i.e. a 2 x 7 table).
```{r, echo = FALSE, message = FALSE}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples","Coffee Ice Cream")) %>%
  group_by(order_dow, product_name) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour) %>%
  knitr::kable(digits = 2)
```
**Table 3**

## Problem 2

```{r, loading and initial cleaning of data, echo = FALSE, message = FALSE}
accel_data = read.csv(file = "./accel_data.csv") %>%
  janitor::clean_names() %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute_of_day",
    values_to = "activity_count") %>%
  mutate(weekend = ifelse(day %in% c("Saturday","Sunday"),1,0)) %>%
  mutate(minute_of_day = str_replace(minute_of_day,"activity_","")) %>%
  mutate(minute_of_day = as.numeric(minute_of_day))
```

After loading and cleaning the accel_data file, the resultant dataset has **`r nrow(accel_data)` rows** and **`r ncol(accel_data)`** columns. The dataset is organized such that accelerometer data ("activity_count") is listed for 35 days by day of week, weekend day status, and chronological minute of day.

```{r, aggregating across minutes to create a total activity variable for each day and creating a table showing these total, message = FALSE, echo = FALSE}
accel_data %>%
  group_by(day_id, weekend) %>%
  summarize(total_activity_count = sum(activity_count)) %>%
  knitr::kable(digits = 0)
```
**Table 4**

The mean activity count over the observed period is `r accel_data %>% pull(activity_count) %>% mean`. It looks like there may be more substantial variation in activity between weekend days versus non weekend days. Some weekends appear to have very low activity while others have high activity.

Figure 1 below shows 24-hour accelerometer data over the course of each day of the week.
```{r, echo = FALSE, message = FALSE}
accel_data %>%
  mutate(day = forcats::fct_relevel(day, c("Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"))) %>%
  ggplot(aes(x = (minute_of_day/60), y = activity_count, color = day)) +
  geom_smooth(se = FALSE) +
  labs(
    title = "Figure 1. Activity Count by Hour of Day",
    x = "Hour of Day",
    y = "Activity Count",
    caption = "35 day observation"
  ) +
 theme_minimal()
```
From this figure, we can see that the patient's activity tends to increase after 5 a.m. and stay moderately high until about 8 p.m., when it then sharply drops off. On Fridays, there is a large amount of activity late in the evening from approximately 8 - 10 p.m. On Sundays there is a large amount of activity mid-day, from approximately 10 a.m. - 12 p.m.


## Problem 3

This problem works with ny_noaa data, which is large, including **`r nrow(ny_noaa)` rows** and **`r ncol(ny_noaa)`** columns. The data is organized by location, data, precipitation value, whether it snowed, snow depth, and maximum and minumum temperature. It is difficult to determine how temperature, precipitation, and snowfall are recorded as their units do not obviously track with standard U.S. estimates. For example, station USC00308600 is recorded with a maximum temperature value of 278 and a minimum temperature value of 189, which does not realistically link to Fahrenheit, Celsius, or Kelvin in an obvious manner. For simplicity, this temperature data is assumed to be 10-fold increases of plausible actual Celsius data. Likewise, precipitation, snowfall, and snow depth are assumed to be 100-fold increases of plausible inches of actual rainfall, snowfall, and snow depth.

```{r, cleaning data, echo = FALSE, message = FALSE}
ny_noaa_clean =
  ny_noaa %>% 
  mutate(year = year(date), month = month(date), day = day(date)) %>%
  mutate(tmax = as.numeric(tmax)) %>%
  mutate(tmin = as.numeric(tmin)) %>%
  mutate(
    tmax = tmax/10,
    tmin = tmin/10,
    prcp = prcp/100,
    snow = snow/100,
    snwd = snwd/100)
```

There is a fair bit of missing data in this dataset. The table below displays the total number of missing variables in each column:
```{r, echo = FALSE}
ny_noaa_clean %>% 
  summarize_all(~ sum(is.na(.))) %>%
  knitr::kable()
```
**Table 5**

Using this histogram, we can see that the most commonly observed value is "0". This is likely because snow only falls occasionally during cold months in a short period of the year.
```{r, echo = FALSE, message = FALSE}
ny_noaa_clean %>%
  ggplot(aes(x = snow)) +
  geom_histogram(bins = 5) +
  labs(
    title = "Figure 2. Histogram of Snow Variable"
  ) +
  theme_minimal()
```

There are thousands of individual weather recording stations. We can display average maximum temperature in January compared to July in each station across years.
```{r, echo = FALSE, message = FALSE}
ny_noaa_clean %>%
  group_by(id, year, month) %>%
  filter(month == 1 | month == 7) %>%
  drop_na(tmax) %>%
  summarize(
    mean_tmax = mean(tmax)) %>%
  ggplot(aes(x = year, y = mean_tmax, color = id)) +
  geom_point(alpha = 0.8) + 
  facet_grid(. ~ month) +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_color_viridis(discrete = TRUE) +
  labs(
    Title = "Figure 3. Average Maximum Temperature in January verus July in Weather Stations in NY, by Year",
    y = "Average Maximum Temperature",
    x = "Year",
    caption = "January is month 1. July is month 7"
  )
```

From this plot, it is obvious that January is much colder than July. There do appear to be some outlier years, such as 1988 when July was relatively cool in one station.